# PyTorch-Transformer-
本项目使用基于PyTorch+Transformer的谣言检测模型，将文本中的谣言事件进行连续向量化，通过一维卷积神经网络的学习训练来挖掘表示文本深层的特征，避免了特征构建的问题，并能发现那些不容易被人发现的特征，从而产生更好的效果。

本项目使用了Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。

对于我们的分类任务来将，我们只需要Transformer中的编码器，解码器是不需要的，Transformer中的编码器可以看成语义提取器，我们将整个句子输入到编码器中然后经过注意力机制以及MLP会对每个字形成一个编码向量，每个编码向量都考虑了不同词对其的影响，这是利用了注意力机制，所以我们可以把整个Encoder的最终输出看成整个语句的编码向量，包含了整个句子的语义信息，然后我们就可以把这些向量喂入全连接网络进行分类。

由于最终会形成词长度个嵌入向量，就是会在每个时间步都形成一个向量，但是我们要将其喂入全连接网络，需要对其进行处理：
  任意取一个标记位置的向量，因为每个词都考虑了所有位置对其的贡献
  将所有词生成的编码向量取均值，然后形成一个编码向量然后喂入输出层
  将所有词的编码向量进行拼接形成一个长的编码向量然后喂入输出层，但是这样会使向量维度特别高，为序列长度*嵌入维度
本项目使用的是取均值向量
